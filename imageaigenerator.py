# -*- coding: utf-8 -*-
"""ImageAIgenerator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cXyUtsU67hko7Z7gPVrQBWQqgBdROHkl
"""

!pip install opencv-python

import torch
print(torch.cuda.is_available())  # Should print True

# Install required libraries
!pip install diffusers transformers accelerate safetensors

# Import required modules
import torch
from diffusers import StableDiffusionPipeline
from PIL import Image

# Load the Stable Diffusion model
model_id = "CompVis/stable-diffusion-v1-4"  # You can also use "stabilityai/stable-diffusion-2"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)
pipe.to("cuda")  # Use GPU for faster processing

# Function to generate an image from a text prompt
def generate_image(prompt, save_path="stable_diffusion_output.png"):
    """Generate and save an image using Stable Diffusion."""
    print(f"Generating image for: {prompt}")
    image = pipe(prompt).images[0]
    image.save(save_path)
    image.show()  # Show image in Colab
    print(f"âœ… Image saved as {save_path}")

# Run the generator
prompt = input("Enter your image prompt: ")
generate_image(prompt)

import os
print(os.listdir("/content/"))  # Lists all files in the Colab working directory

from google.colab import files
files.download("/content/stable_diffusion_output.png")

import torch
from diffusers import StableDiffusionPipeline

# Load the model (make sure to use "cpu" if you don't have a GPU with CUDA support)
model_id = "CompVis/stable-diffusion-v1-4"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)

# Use CPU if you don't have CUDA
device = "cuda" if torch.cuda.is_available() else "cpu"
pipe.to(device)

# New prompt for generating a fresh image
new_prompt = "A dog playing with a ball in open ground"

# Generate a new image
image = pipe(new_prompt).images[0]

# Save the new image
image.save("new_stable_diffusion_output.png")
print("New image saved as new_stable_diffusion_output.png")

from google.colab import files

# Download the image
files.download("new_stable_diffusion_output.png")

!pip install flask pyngrok

# Install Flask and Flask-CORS for the API, and pyngrok to create a public tunnel
!pip install Flask Flask-Cors pyngrok

# Essential libraries for your Stable Diffusion model (already in your notebook)
!pip install diffusers transformers accelerate safetensors opencv-python torch

import torch
from diffusers import DiffusionPipeline
from flask import Flask, send_from_directory, jsonify, request
from flask_cors import CORS
import os
import uuid
from PIL import Image # Used for saving images

# Check for CUDA availability (ensure GPU is connected in Colab)
print(f"CUDA available: {torch.cuda.is_available()}")

# Load the Stable Diffusion pipeline once globally
# Using float16 for memory efficiency on GPU
try:
    pipe = DiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16)
    if torch.cuda.is_available():
        pipe.to("cuda")
    print("Stable Diffusion model loaded successfully.")
except Exception as e:
    print(f"Error loading model: {e}")
    pipe = None # Set to None if loading fails

def generate_image_from_prompt(prompt_text: str):
    if pipe is None:
        raise RuntimeError("Stable Diffusion model not loaded. Cannot generate image.")

    # You can add more parameters here like num_inference_steps, guidance_scale etc.
    image = pipe(prompt_text).images[0]

    # Define a directory to save images within Colab's temporary storage
    IMAGES_DIR = '/content/generated_images'
    if not os.path.exists(IMAGES_DIR):
        os.makedirs(IMAGES_DIR)

    # Generate a unique filename for the image
    image_filename = f"sd_output_{uuid.uuid4()}.png"
    image_path = os.path.join(IMAGES_DIR, image_filename)

    image.save(image_path)
    print(f"Image saved to: {image_path}")
    return image_filename # Return only the filename

app = Flask(__name__)
CORS(app) # Enable Cross-Origin Resource Sharing (CORS) for your React app

# Directory for generated images (must match the one in generate_image_from_prompt)
IMAGES_DIR = '/content/generated_images'
if not os.path.exists(IMAGES_DIR):
    os.makedirs(IMAGES_DIR)

@app.route('/generate', methods=['POST'])
def generate_image_api():
    data = request.json
    prompt = data.get('prompt', 'a photo of an astronaut riding a horse on mars') # Default prompt

    if not prompt:
        return jsonify({"error": "Prompt is required"}), 400

    try:
        generated_filename = generate_image_from_prompt(prompt)
        # The URL will be relative to the ngrok tunnel's public URL
        return jsonify({
            "message": "Image generated successfully!",
            "image_filename": generated_filename,
            "image_url": f"/images/{generated_filename}" # Endpoint for serving files
        }), 200
    except RuntimeError as e:
        return jsonify({"error": str(e)}), 500
    except Exception as e:
        return jsonify({"error": f"An unexpected error occurred: {str(e)}"}), 500

@app.route('/images/<filename>')
def serve_image(filename):
    return send_from_directory(IMAGES_DIR, filename)

from pyngrok import ngrok
import threading
import time

# Terminate any existing ngrok tunnels to avoid conflicts
ngrok.kill()

# IMPORTANT: Get your ngrok authentication token from https://ngrok.com/signup
# and paste it here or set it as an environment variable.
# ngrok.set_auth_token("YOUR_NGROK_AUTH_TOKEN") # Uncomment and set your token

# Function to run Flask in a separate thread
def run_flask():
    app.run(port=5000)

# Start Flask in a new thread
flask_thread = threading.Thread(target=run_flask)
flask_thread.start()

# Give Flask a moment to start up
time.sleep(2)

# Connect ngrok to the Flask port (5000)
try:
    public_url = ngrok.connect(5000).public_url
    print(f"Flask app running locally on: http://localhost:5000")
    print(f"Public URL for your Flask app (use this in React): {public_url}")
    print("Keep this Colab cell running to keep the server active.")
except Exception as e:
    print(f"Error starting ngrok or Flask: {e}")
    print("Please ensure your ngrok auth token is set and you have available tunnels.")